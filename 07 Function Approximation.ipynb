{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximation\n",
    "\n",
    "For large state spaces, it is not feasible to store value functions in tabular form\n",
    "* The number of states can be infinitely many (Ex: Camera image as state).\n",
    "* Memory is limited, and we cannot store values for a large number of states.\n",
    "* Time and data are also limited\n",
    "* It is possible that many states are never visited.\n",
    "\n",
    "This can be addressed by using *parameterized functions* for approximating the value functions.\n",
    "\n",
    "\n",
    "## Linear value-function approximation\n",
    "\n",
    "Let $\\hat{v}$ represent an approximate state-value function.\n",
    "\n",
    "$$ \\hat{v}(s,\\textbf{w}) \\doteq \\sum_i w_i x_i(s) $$\n",
    "\n",
    "$$  \\hat{v}(s,\\textbf{w}) \\doteq \\textbf{w}^\\text{T}\\textbf{x}(s) $$\n",
    "\n",
    "where\n",
    "\\\n",
    "$\\textbf{w}$ $-$ weight vector\n",
    "\\\n",
    "$\\textbf{x}(s)$ $-$ feature vector\n",
    "\\\n",
    "$x_i(s)$ $-$ $i^{th}$ feature\n",
    "\\\n",
    "$w_i$ $-$ $i^{th}$ weight\n",
    "\n",
    "$\\textbf{w}$ and $\\textbf{x}(s)$ are of the same dimentions.\n",
    "\n",
    "#### Tabular value functions as Linear functions\n",
    "\n",
    "If we use one-hot vectors as feature vectors, the weights corresponding to the $1$ in the vector becomes the value of the state it represents.\n",
    "\n",
    "**INSERT TABLE FOR EXAMPLE**\n",
    "\n",
    "$$\\textbf{x}(s_1) = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "0\\\\ \n",
    "0\n",
    "\\end{bmatrix}\\ \\ \\ \n",
    "\\textbf{x}(s_2) = \\begin{bmatrix}\n",
    "0\\\\ \n",
    "1\\\\ \n",
    "0\n",
    "\\end{bmatrix}\\ \\ \\ \n",
    "\\textbf{x}(s_3) = \\begin{bmatrix}\n",
    "0\\\\ \n",
    "0\\\\ \n",
    "1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\hat{v}(s_1, \\textbf{w}) = w_1\\ \\ \\ \\hat{v}(s_2, \\textbf{w}) = w_2\\ \\ \\ \\hat{v}(s_3, \\textbf{w}) = w_3$$\n",
    "\n",
    "#### Generalization and Discrimination\n",
    "\n",
    "We need to do better than one-hot vectors for features. We want two properties in the features we choose:\n",
    "\n",
    "* High __Generalization__ - representing multiple states using their common features\n",
    "* High __Discrimination__ - being able to identify different states based on their differences\n",
    "\n",
    "\n",
    "## The Prediction Objective\n",
    "\n",
    "In the tabular case, the learned value could become equal to the true value, and the values of every state were decoupled\n",
    "\n",
    "With function approximation, we'll have more states than weights, the states are generalized, and changing the value of one state affects others state-values.\n",
    "\n",
    "We must specify a distribution that describes how much we care about the error in each state.\n",
    "\n",
    "$$ \\mu(s) \\geq 0, \\sum_s \\mu(s) = 1 $$\n",
    "\n",
    "Using this, we describe the *Mean Squared Value Error* as:\n",
    "\n",
    "$$ \\overline{\\text{VE}} = \\sum_s \\mu(s) \\left [ v_{\\pi}(s) - \\hat{v}(s,\\textbf{w}) \\right ]^2 $$\n",
    "\n",
    "We want to minimize this error. We could use gradient descent...\n",
    "\n",
    "$$ \\bigtriangledown \\overline{\\text{VE}} = \\bigtriangledown  \\sum_s \\mu(s) \\left [ v_{\\pi}(s) - \\hat{v}(s,\\textbf{w}) \\right ]^2 $$\n",
    "\n",
    "$$ \\bigtriangledown \\overline{\\text{VE}} = \\sum_s \\mu(s) \\bigtriangledown \\left [ v_{\\pi}(s) - \\hat{v}(s,\\textbf{w}) \\right ]^2 $$\n",
    "\n",
    "$$ \\bigtriangledown \\overline{\\text{VE}} = \\sum_s \\mu(s). 2.\\left [ v_{\\pi}(s) - \\hat{v}(s,\\textbf{w}) \\right ] \\bigtriangledown \\hat{v}(s, \\textbf{w}) $$\n",
    "\n",
    "## Monte Carlo with Function Approximation\n",
    "\n",
    "The equation above is fine, but we don't know $\\mu(s)$ because we only get the states online. Enter *Monte Carlo*! We __sample states__ and use the samples to approximate the weights.\n",
    "\n",
    "Let's assume that we have access to $v_{\\pi}$ values and we sample states: $ \\left ( S_1, v_{\\pi}(S_1) \\right ), \\left ( S_2, v_{\\pi}(S_2) \\right ), \\ldots $\n",
    "\n",
    "The *Stochastic Gradient Descent update* for one weight can be written as:\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha \\left [ v_{\\pi}(S_1) - \\hat{v}(S_1, \\textbf{w}_1) \\right ] \\bigtriangledown \\hat{v}(S, \\textbf{w}) $$\n",
    "\n",
    "But we don't actually know $v_{\\pi}$. So we will have to use the expected return $G_t$ here instead.\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha \\left [ G_t - \\hat{v}(S_1, \\textbf{w}_1) \\right ] \\bigtriangledown \\hat{v}(S, \\textbf{w}) $$\n",
    "\n",
    "Let's rewrite the MC SGD update equation with a generic notation for a target $U_t$:\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha \\left [ U_t - \\hat{v}(S_1, \\textbf{w}_1) \\right ] \\bigtriangledown \\hat{v}(S, \\textbf{w}) $$\n",
    "\n",
    "If $U_t$ is an unbiased estimate of the true value, then $\\textbf{w}$ will converge to a local optimum.\n",
    "\n",
    "## TD with Function Approximation\n",
    "\n",
    "Like in the tabular case, Monte Carlo methods usng Function Approximation will require episode completion before the weights can be updated (and through them the state-values). We can do better with Temporal Difference learning.\n",
    "\n",
    "We can use the one-step TD target:\n",
    "\n",
    "$$ U_t \\doteq R_{t+1} + \\gamma . \\hat{v}(S_{t+1}, \\textbf{w}) $$\n",
    "\n",
    "By bootstrapping targets, we use the current value of $\\textbf{w}$, like in $\\hat{v}(S_{t+1}, \\textbf{w})$, and this makes $U_t$ biased.\n",
    "\n",
    "Bootstrapping methods are not instances of true gradient descent. These are called __semi-gradient__ methods because they take into account the effects of changes in the weights for the estimate $\\hat{v}(S_1, \\textbf{w}_1)$ but not the target $U_t$.\n",
    "\n",
    "$$ \\bigtriangledown \\left [ U_t - \\hat{v}(S_t, \\textbf{w}) \\right ]^2 = \\left ( U_t - \\hat{v}(S_t, \\textbf{w}) \\right ) \\left ( \\bigtriangledown U_t - \\bigtriangledown \\hat{v}(S_t, \\textbf{w}) \\right) $$\n",
    "\n",
    "This update term becomes the gradient descent update term when\n",
    "\n",
    "$$ \\bigtriangledown U_t = 0 $$\n",
    "\n",
    "But, for one-step TD, \n",
    "\n",
    "$$ \\bigtriangledown U_t = \\gamma . \\bigtriangledown \\hat{v}(S_{t+1}, \\textbf{w}) \\neq 0 $$ \n",
    "\n",
    "i.e. it depends on $\\textbf{w}$\n",
    "\n",
    "Semi-gradient methods are not as robust as the gradient methods. So __why do we prefer Semi-gradient methods__?\n",
    "* They perform well with linear functions\n",
    "* They enable faster learning\n",
    "* Allow learning to be continuous and online (not limited to episodic tasks)\n",
    "* Computational advantages\n",
    "\n",
    "## Semi-gradient methods with Linear function approximation\n",
    "\n",
    "Recall the update equation for weights\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha \\left [ U_t - \\hat{v}(S_1, \\textbf{w}_1) \\right ] \\bigtriangledown \\hat{v}(S, \\textbf{w}) $$\n",
    "\n",
    "For $ \\bigtriangledown U_t = \\gamma \\bigtriangledown \\hat{v}(S_{t+1}, \\textbf{w}) $, we can rewrite the update equation as:\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\delta_t \\bigtriangledown \\hat{v}(S, \\textbf{w}) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\delta_t \\doteq R_{t+1} + \\gamma . \\hat{v}(S_{t+1},\\textbf{w}) - \\hat{v}(S_{t},\\textbf{w}) $$\n",
    "\n",
    "If we use linear approximation functions:\n",
    "\n",
    "$$ \\hat{v}(S_t,\\textbf{w}) \\doteq \\textbf{w}^\\text{T}\\textbf{x}(S_t) $$\n",
    "\n",
    "$$ \\bigtriangledown \\hat{v}(S_t,\\textbf{w}) = \\textbf{x}(S_t) $$\n",
    "\n",
    "The weight update equation now becomes:\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\delta_t \\textbf{x}(S_t) $$\n",
    "\n",
    "Let $\\textbf{x}_t = \\textbf{x}(S_t)$,\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\delta_t \\textbf{x}_t $$\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\left [ R_{t+1} + \\gamma . \\hat{v}(S_{t+1},\\textbf{w}) - \\hat{v}(S_{t},\\textbf{w}) \\right ] \\textbf{x}_t $$\n",
    "\n",
    "Expand $ \\hat{v} $,\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\left [ R_{t+1} + \\gamma . \\textbf{w}^\\text{T}\\textbf{x}_{t+1} - \\textbf{w}^\\text{T}\\textbf{x}_t \\right ] \\textbf{x}_t $$\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\left [ R_{t+1} \\textbf{x}_t - \\textbf{x}_t (\\textbf{x}_t - \\gamma\\ \\textbf{x}_{t+1} )^\\text{T}\\ \\textbf{w} \\right ] $$\n",
    "\n",
    "#### Update Expectation\n",
    "\n",
    "Let vector $ \\textbf{b} \\doteq R_{t+1} \\textbf{x}_t$\n",
    "\n",
    "and matrix $ \\textbf{A} \\doteq \\textbf{x}_t (\\textbf{x}_t - \\gamma\\ \\textbf{x}_{t+1} )^\\text{T} $\n",
    "\n",
    "The expected update can be written as\n",
    "\n",
    "$$ \\mathop{{}\\mathbb{E}}  \\left [ \\Delta \\textbf{w}_t  \\right ] = \\alpha \\left ( \\textbf{b} - \\textbf{Aw}^\\text{T} \\right ) $$\n",
    "\n",
    "The weights are said to converge when $ \\mathop{{}\\mathbb{E}}  \\left [ \\Delta \\textbf{w}_t  \\right ] = 0 $.\n",
    "\n",
    "The weights at this *TD Fixed Point* are given by:\n",
    "\n",
    "$$ \\textbf{w}_{\\text{TD}} = \\textbf{A}^{-1}\\textbf{b} $$\n",
    "\n",
    "At this TD fixed point, $\\overline{\\text{VE}}$ is within a bounded expansion of the lowest possible error.\n",
    "\n",
    "$$ \\overline{\\text{VE}}\\ (\\textbf{w}_{\\text{TD}}) \\leq \\frac{1}{1-\\gamma} \\underset{\\text{w}}{min}\\ \\overline{\\text{VE}}\\ (\\text{w}) $$\n",
    "\n",
    "Since we use $\\gamma$ values near one, this bound is usually large. \n",
    "\\\n",
    "But, while MC methods may diverge, TD methods converge faster since they have less variance.\n",
    "\n",
    "\n",
    "\n",
    "## Feature construction\n",
    "\n",
    "* Features are an important way of adding domain knowledge into Reinforcement Learning systems.\n",
    "* Since in linear methods there is not interaction between features, we must construct features that are a combinations of the state dimensions.\n",
    "* Features should allow high generalization and discrimination.\n",
    "\n",
    "#### Some general methods of Feature Construction:\n",
    "\n",
    "* __Polynomials__: Combinations of the state dimensions. The polynomial coefficients will be the weights.\n",
    "* __Fourier Bases__: Any periodic function can be expressed as a weighted sum of sines and cosines of different frequencies.\n",
    "* __Coarse coding__: Use N-dimensional shapes as features. Overlapping features like this provide generalization if the shapes are large and discrimination at the intersections.\n",
    "* __Tile coding__: It is hard to implement coarse coding (arbitrary shapes in N-dimensions), so this strategy uses repeating grids offset by an amount much smaller than the grid size.\n",
    "* __Neural Networks__: Let the network learn to identify useful features during training.\n",
    "\n",
    "\n",
    "## Control with Function Approximation\n",
    "\n",
    "For control, we need action-value fucntions. We have been estimating state-value functions thus far. We can estimate the action-values in the same way.\n",
    "\n",
    "We know,\n",
    "$$  \\hat{v}(s,\\textbf{w}) \\doteq \\textbf{w}^\\text{T}\\textbf{x}(s) $$\n",
    "Similarly,\n",
    "$$  \\hat{q}(s,a,\\textbf{w}) \\doteq \\textbf{w}^\\text{T}\\textbf{x}(s,a) $$\n",
    "\n",
    "### Sarsa\n",
    "\n",
    "The weight update for TD learning was,\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\left [ R_{t+1} + \\gamma . \\hat{v}(S_{t+1},\\textbf{w}) - \\hat{v}(S_{t},\\textbf{w}) \\right ] \\bigtriangledown \\hat{v}(S_t, \\textbf{w}) $$\n",
    "\n",
    "For TD with GPI (Sarsa), it becomes,\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\left [ R_{t+1} + \\gamma . \\hat{q}(S_{t+1},A_{t+1},\\textbf{w}) - \\hat{q}(S_{t},A_{t},\\textbf{w}) \\right ] \\bigtriangledown \\hat{q}(S_t, A_t, \\textbf{w}) $$\n",
    "\n",
    "Note the similarity to the Tabular Sarsa update rule:\n",
    "\n",
    "$$ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left [ R_{t+1} + \\gamma . Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \\right ] $$\n",
    "\n",
    "### Expected Sarsa\n",
    "\n",
    "Compute the expectation of the next action's value using the current policy (instead of sampling actions from the policy):\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\left [ R_{t+1} + \\gamma . \\sum_{a'} \\pi (a'|S_{t+1})\\ \\hat{q}(S_{t+1},a',\\textbf{w}) - \\hat{q}(S_{t},A_{t},\\textbf{w}) \\right ] \\bigtriangledown \\hat{q}(S_t, A_t, \\textbf{w}) $$\n",
    "\n",
    "$$ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left [ R_{t+1} + \\gamma . \\sum_{a'} \\pi (a'|S_{t+1}) . Q(S_{t+1},a') - Q(S_t,A_t) \\right ] $$\n",
    "\n",
    "### Q-learning\n",
    "\n",
    "Off-policy learning that assumes greedy action selection in the behavior policy.\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha\\ \\left [ R_{t+1} + \\gamma . \\underset{a'}{max}\\ \\hat{q}(S_{t+1},a',\\textbf{w}) - \\hat{q}(S_{t},A_{t},\\textbf{w}) \\right ] \\bigtriangledown \\hat{q}(S_t, A_t, \\textbf{w}) $$\n",
    "\n",
    "$$ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left [ R_{t+1} + \\gamma . \\underset{a'}{max}\\ Q(S_{t+1},a') - Q(S_t,A_t) \\right ] $$\n",
    "\n",
    "\n",
    "## Exploration and exploitation in Function Approximation\n",
    "\n",
    "### Optimistic initialization\n",
    "\n",
    "* Set optimistic weights $\\textbf{w}$.\n",
    "* We cannot know the optimistic weights in non-linear systems (Neural Nets)\n",
    "* Due to generalization, optimism can be lost quickly before some states are even visited.\n",
    "\n",
    "### $\\epsilon-$greedy action selection\n",
    "\n",
    "* Works for function approximation as well as it did for tabular methods.\n",
    "* Not systematic\n",
    "\n",
    "## The Deadly Triad\n",
    "\n",
    "The usage of the following three methods together could lead to instability in the system.\n",
    "\n",
    "* __Function Approximation__ (Ex: Linear function approximation, Artificial Neural Nets).\n",
    "  * Can't let go of this one.\n",
    "  \n",
    "\n",
    "* __Bootstrapping__: Using existing estimates in targets (Dynamic Programming, Temporal Difference).\n",
    "  * Costs memory to let go of bootstrapping, but MC methods could be used.\n",
    "  * Learning takes more time without bootstrapping.\n",
    "\n",
    "\n",
    "* __Off-policy learning__: Training on a distribution of transitions other than that produced by the target policy.\n",
    "  * In large-scale learning, with many agents learning in parallel, Off-policy learning becomes essential.\n",
    "  * On-policy methods are adequate in small solutions\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
