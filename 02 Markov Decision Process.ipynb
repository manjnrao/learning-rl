{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "MDPs are a classical formalization of sequential decision making, where actions influence not just *immediate rewards*, but also *subsequent situations*, or *states*, and through those influence future rewards.\n",
    "\n",
    "A state is said to have the __Markov property__ if it contains all the information required from previous states, so that we can make decisions relying only on the current state.\n",
    "\n",
    "\n",
    "## Agent and Environment\n",
    "\n",
    "In MDPs the learner/decision maker is called the __Agent__.\n",
    "\n",
    "Everything outside the agent in the world is the __Environment__.\n",
    "\n",
    "Generally anything that the *agent* cannot change is considered part of the *environment*. For example, a robot's body or even the actuators in it are not part of the agent, but they belong to the *environment*. The signal to the actuators are controlled by the agent but the actuators can have noisy behavior, which is why they are part of the *environment* and not the *agent*.\n",
    "\n",
    "The *agent* and the *environment* interact continually. The *agent* selects actions, and the *environment* returns subsequent states and the reward for selecting an action. This process continues until termination.\n",
    "\n",
    "The trajectory looks like: $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\ldots$\n",
    "\n",
    "## Finite MDPs\n",
    "\n",
    "* When States, Actions and Rewards are all finite.\n",
    "* We can define the *dynamics* of a finite MDP as:\n",
    "  $$ p(s',r|s,a) \\doteq Pr\\left \\{ S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a \\right \\} $$\n",
    "  \n",
    "  It gives us the probability of a certain reward and the next state given a state and action at the current timestep.\n",
    "  \n",
    "\n",
    "* MDP dynamics has this property:\n",
    "  $$\\sum_{s'\\in S} \\sum_{r\\in R} p(s',r|s,a) = 1,\\ \\ \\forall s\\in S, a\\in A(s) $$\n",
    "  \n",
    "## Goals *vs* Rewards\n",
    "\n",
    "__Reward__ is a number given by the environment that the agent tries to maximize.\n",
    "\n",
    "The agent's __goal__ ,or purpose, is to maximize the *total reward* it receives.\n",
    "\n",
    "It is important that the rewards are setup to encourage the desired behavior specifically, and not to impart prior knowledge.\n",
    "\\\n",
    "If rewards are not set up properly the agent might maximize rewards but not learn what we want it to.\n",
    "\n",
    "## Episodes and Returns\n",
    "\n",
    "Tasks that terminate are called *episodic tasks*. Each iteration of this task is called an *Episode*.\n",
    "\\\n",
    "In contrast, there are *continuing tasks* that do not terminate.\n",
    "\n",
    "In an episodic task that terminates at timestep $T$, the *expected return* $G$ at timestep $t$ during the task can be written as:\n",
    "\n",
    "$$ G_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + \\ldots + R_{T} $$\n",
    "\n",
    "In continuing tasks, there is no termination, and adding rewards as shown above results in infinite return. So we introduce a __discount factor $\\gamma$__ and write the return at timestep $t$ as:\n",
    "\n",
    "$$ G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots $$\n",
    "\n",
    "$$ G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$\n",
    "\n",
    "Discount factor (or discount rate) $\\gamma \\in [0,1]$ represents the current value of future rewards.\n",
    "\\\n",
    "$\\gamma = 0$ means we don't care about the future.\n",
    "\\\n",
    "$\\gamma = 1$ means future rewards have the most value in the present state. (0.99 is better since 1 results in $G = \\infty$)\n",
    "\n",
    "## Policies\n",
    "\n",
    "A __policy__ is a mapping from *states* to probabilities of selecting each possible *action*.\n",
    "\n",
    "If an agent is following a policy $\\pi$, then $\\pi(a|s)$ is the probability that action $a$ is selected while in state $s$.\n",
    "\n",
    "There are two kinds of policies:\n",
    "* __Deterministic policy__: maps states to specific action selection.\n",
    "* __Stochastic policy__: maps states to a probability of action selection.\n",
    "\n",
    "\n",
    "## Value functions\n",
    "\n",
    "There are two kinds of *value functions*: \n",
    "* __State value functions $v_{\\pi}(s)$ $-$__ estimate \"how good\" it is to be in a particular state. \n",
    "* __Action value functions $q_{\\pi}(s,a)$ $-$__ estimate \"how good\" it is to choose an action while in a state.\n",
    "\n",
    "In other words, they estimate expected return.\n",
    "\n",
    "$$ v_{\\pi}(s) \\doteq \\mathop{{}\\mathbb{E}_{\\pi}} \\left[ G_t | S_t = s \\right ] $$\n",
    "\n",
    "$$ q_{\\pi}(s,a) \\doteq \\mathop{{}\\mathbb{E}_{\\pi}} \\left[ G_t | S_t = s, A_t = a \\right ] $$\n",
    "\n",
    "\\* the $\\pi$ subscript in $v_{\\pi}(s)$, $q_{\\pi}(s,a)$ and $\\mathop{{}\\mathbb{E}_{\\pi}}$ means \"*while following the policy $\\pi$*\" or \"*under policy $\\pi$*\".\n",
    "\n",
    "\\* In the *bandit problem* we estimated the value $q(a)$ of each action $a$.\n",
    "\\\n",
    "$\\ \\ $ In MDPs, we estimate the value $q(s, a)$ of each action $a$ in each state $s$, or we estimate the value $v(s)$ of each state given optimal action selections.\n",
    "\n",
    "## Bellman equations\n",
    "\n",
    "The value functions can be written recursively. We start with the expected return $G_t$ that can be written recursively as:\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma G_{t+1} $$\n",
    "\n",
    "So, $v_{\\pi}(s)$ can be written as:\n",
    "$$ v_{\\pi}(s) \\doteq \\mathop{{}\\mathbb{E}_{\\pi}} \\bigl[ R_{t+1} + \\gamma G_{t+1} | S_t = s \\bigr ] $$\n",
    "\n",
    "Expanding the expectation for all *actions*, and the resulting *rewards* and *subsequent states*, we get:\n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_a \\pi (a|s) \\sum_{s'} \\sum_r p(s',r | s,a) \\bigl [ r + \\gamma \\mathop{{}\\mathbb{E}_{\\pi}} \\left[ G_{t+1} | S_{t+1} = s \\right ] \\bigr ] $$\n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_a \\pi (a|s) \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . v_{\\pi}(s') \\right ] $$\n",
    "\n",
    "Similarly for $q_{\\pi}(s,a)$ we have:\n",
    "$$ q_{\\pi}(s,a) \\doteq \\mathop{{}\\mathbb{E}_{\\pi}} \\left[ G_t | S_t = s, A_t = a \\right ] $$\n",
    "\n",
    "$$ q_{\\pi}(s,a) = \\sum_{s'} \\sum_r p(s',r | s,a) \\left [ r + \\gamma \\mathop{{}\\mathbb{E}_{\\pi}} \\left [ G_{t+1} | S_{t+1} = s, A_t = a \\right ] \\right ] $$\n",
    "\n",
    "$$ q_{\\pi}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\sum_{a'} \\pi (a'|s') . q_{\\pi}(s',a') \\right ] $$\n",
    "\n",
    "## Optimal Policies\n",
    "\n",
    "__Optimal Policies__ are those that have the highest possible value function in all states.\n",
    "\n",
    "* A policy is better than or equal to another policy if and only if its expected return is better than or equal to the expected return of the other policy in all states.\n",
    "  $ \\pi \\geq \\pi ' \\text{ iff } v_{\\pi}(s) \\geq v_{\\pi '}(s) \\forall s \\in S $\n",
    "* There exists at least one optimal policy.\n",
    "* If two policies are such that one gives highest return in half the states and the other gives highest return in the other half, then a new policy can be constructed using these, and it will be the optimal policy.\n",
    "  \n",
    "## Optimal value functions\n",
    "\n",
    "__Optimal value functions__ are value functions that estimate the expected return of optimal policies.\n",
    "\n",
    "* All optimal policies share the same value function\n",
    "\n",
    "  $$ v_*(s) \\doteq \\underset{\\pi}{max}\\ v_{\\pi}(s) $$\n",
    "\n",
    "  $$ q_*(s,a) \\doteq \\underset{\\pi}{max}\\ q_{\\pi}(s,a) $$\n",
    "  \n",
    "* We can write $q_*$ in terms of $v_*$:\n",
    "\n",
    "  $$ q_*(s,a) = \\mathop{{}\\mathbb{E}} \\left[ R_{t+1} + \\gamma v_*(s_{t+1}) | S_t = s, A_t = a \\right ] $$\n",
    "\n",
    "\n",
    "* We can also write $v_*$ in terms of $q_*$:\n",
    "\n",
    "  $$ v_*(s) = \\underset{a \\in A}{max}\\ q_{\\pi_*}(s,a) $$\n",
    "  \n",
    "\n",
    "* We can obatin Optimal Policies from Optimal Value functions as follows:\n",
    "  - From *state value* functions\n",
    "  $$ \\pi_*(s) = \\underset{a}{argmax}\\ \\sum_{s',r}p(s',r|s,a) \\left [ r + \\gamma . v_*(s') \\right ] $$\n",
    "\n",
    "  - From *action value* functions\n",
    "  $$ \\pi_*(s) = \\underset{a}{argmax}\\ q_*(s,a) $$\n",
    "  \n",
    "## Bellman Optimality Equations\n",
    "\n",
    "We know $v_{\\pi}(s)$:\n",
    "$$ v_{\\pi}(s) = \\sum_a \\pi (a|s) \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . v_{\\pi}(s') \\right ] $$\n",
    "\n",
    "For $\\pi_*$,\n",
    "$$ v_{\\pi_*}(s) = \\sum_a \\pi_* (a|s) \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . v_{\\pi_*}(s') \\right ] $$\n",
    "\n",
    "In an optimal policy the best action gets a probability of 1 and others 0.\n",
    "\n",
    "$$ v_{\\pi_*}(s) = \\underset{a}{max} \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . v_{\\pi_*}(s') \\right ] $$\n",
    "\n",
    "Similarly, for $q_{\\pi_*}(s)$ we have:\n",
    "$$ q_{\\pi_*}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\underset{a'}{max} . q_{\\pi_*}(s',a') \\right ] $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
