{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "Dynamic Programming provides a foundation to understand more complex techniques used in Reinforcement Learning. Given the perfect model of the environment, DP can be used to compute optimal policies.\n",
    "\n",
    "* We usually do DP on __finite MDPs__, i.e., states, actions and rewards are all finite.\n",
    "* We assume to have the perfect model of the environment, i.e., we assume that we know the MDP dynamics: $ p(s',r|s,a) $.\n",
    "* We can obtain the optimal policies once we have the optimal value functions, and we use DP to calculate the optimal value functions.\n",
    "* We use Bellman equations to write update rules that improve the approximations of the desired value functions.\n",
    "\n",
    "Dynamic Programming is costly, computationally, and also with respect to memory utilization.\n",
    "\\\n",
    "It is also limited because of the assumption of a perfect model of the environment.\n",
    "\n",
    "## Policy Evaluation (Prediction)\n",
    "\n",
    "__Policy evaluation__ is computing the value functions $v_{\\pi}(s)$ or $q_{\\pi}(s,a)$ for an arbitrary policy $\\pi$.\n",
    "\n",
    "We start by initilaizing arbitrary values for each state and use the Bellman equation to update these values iteratively. As the number of iterations get larger, the value function estimate becomes more accurate.\n",
    "\n",
    "Recall Bellman equations for value functions,\n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_a \\pi (a|s) \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . v_{\\pi}(s') \\right ] $$\n",
    "\n",
    "$$ q_{\\pi}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\sum_{a'} \\pi (a'|s') . q_{\\pi}(s',a') \\right ] $$\n",
    "\n",
    "Let $k$ denote iterations. The value function update for the $(k+1)^{th}$ update is given as:\n",
    "\n",
    "$$ v_{k+1}(s) = \\sum_a \\pi (a|s) \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . v_{k}(s') \\right ] $$\n",
    "\n",
    "$$ q_{k+1}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\sum_{a'} \\pi (a'|s') . q_{k}(s',a') \\right ] $$\n",
    "\n",
    "As $k \\rightarrow \\infty$, $v_k$ and $q_k$ approache $v_{\\pi}$ and $q_{\\pi}$ respectively.\n",
    "\n",
    "While updating the value function, we can do the updates in place rather than use two arrays to keep the old and the new estimates separate, to converge faster.\n",
    "\n",
    "## Policy Improvement\n",
    "\n",
    "After we evaluate the value functions for a policy $\\pi$, we can use these values to create a possibly better policy $\\pi'$ as shown below:\n",
    "\n",
    "$$ \\pi'(s) = \\underset{a}{argmax}\\ \\sum_{s',r}p(s',r|s,a) \\left [ r + \\gamma . v_{\\pi}(s') \\right ] $$\n",
    "  \n",
    "$$ \\pi'(s) = \\underset{a}{argmax}\\ q_{\\pi}(s,a) $$\n",
    "\n",
    "If $\\pi'$ is better than $\\pi$, then for all states $s \\in S$:\n",
    "\\\n",
    "$\\ \\ \\ \\ \\ v_{\\pi'}(s) \\geq v_{\\pi}(s) $\n",
    "\\\n",
    "$\\ \\ \\ \\ \\ q_{\\pi'}(s,a) \\geq v_{\\pi}(s,a) $\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "Once we have an improved policy $\\pi'$ from $v_{\\pi}(s)$, we can repeat the process, and find $v_{\\pi'}(s)$ before updating the policy again. \n",
    "\n",
    "This process of performing *__policy evaluation and improvement iteratively__* is called Policy iteration.\n",
    "\n",
    "Only the __optimal policy is greedy with respect to its own value function__, i.e., the policy obtained by using the value function which was calculated using the optimal policy is the optimal policy itself.\n",
    "\n",
    "## Value Iteration\n",
    "\n",
    "A drawback of policy iteration is the policy evaluation part. We iterate $k$ times to obtain the value function, but $v_k$ is only guaranteed to approach $v_{\\pi}$ as $k \\rightarrow \\infty$.\n",
    "\n",
    "Value iteration combines a truncated policy evaluation (*by only doing one sweep of value function updates on all states*) and then policy improvement using the Bellman Optimality equation as the update rule.\n",
    "\n",
    "Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation\n",
    "and one sweep of policy improvement.\n",
    "\n",
    "Value Iteration Update rules\n",
    "$$ v_{k+1}(s) = \\underset{a}{max} \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . v_{k}(s') \\right ] $$\n",
    "$$ q_{k+1}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\underset{a'}{max} . q_{k}(s',a') \\right ] $$\n",
    "\n",
    "Bellman Optimality Equations (for comparison)\n",
    "$$ v_{\\pi_*}(s) = \\underset{a}{max} \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . v_{\\pi_*}(s') \\right ] $$\n",
    "$$ q_{\\pi_*}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\underset{a'}{max} . q_{\\pi_*}(s',a') \\right ] $$\n",
    "\n",
    "## Generalized Policy Iteration (GPI)\n",
    "\n",
    "Policy iteration consists of policy evaluation and policy improvement. Each is completed before the other begins. In value iteration, one iteration of the policy evaluation is performed between each policy improvement step. Generalized Policy Iteration refers to the idea of letting policy evaluation and policy improvement interact, independent of each other's implementation details.\n",
    "\n",
    "The diagram below shows how the policy and value function convergence to optimality while following GPI.\n",
    "\n",
    "![title](https://miro.medium.com/max/2624/1*udhphWhqjadT-osAQhL6AQ.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
