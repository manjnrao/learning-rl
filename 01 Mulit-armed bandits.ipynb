{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Reinforcement Learning is learning what to do $-$ mapping states to actions $-$ to maximize a reward signal. It is different from *supervised learning* because there are no labels or *right answers* given to the system. Training information is used by the *agent* to evaluate its actions (the correct / optimal actions are not provided), and the agent needs to explore the problem actively to search for better and better actions.\n",
    "\n",
    "## The k-armed Bandit Problem\n",
    "\n",
    "Consider the following problem:\n",
    "* You have k options to choose from / actions to take.\n",
    "* You get a reward corresponding to chosen action.\n",
    "* Rewards are chosen from a stationary probability distribution.\n",
    "\n",
    "The objective is to maximize the total reward over a time period.\n",
    "\n",
    "## Action Values\n",
    "\n",
    "In order to solve the bandit problem, we need to know the value of each action. We represent this as a *q-function* or *action-value function*.\n",
    "\n",
    "$a$ $-$ action\n",
    "\\\n",
    "$t$ $-$ timestep\n",
    "\\\n",
    "$A_t$ $-$ action selected at timestep $t$\n",
    "\\\n",
    "$R_t$ $-$ reward received at timestep $t$\n",
    "\\\n",
    "$q_*(a)$ $-$ expected reward given that action $a$ is selected\n",
    "\n",
    "$$ q_*(a) \\doteq \\mathop{{}\\mathbb{E}}[R_t | A_t = a] $$\n",
    "\n",
    "If we know the value of each action, we can simply select the *greedy action* (action with highest reward) every time. \n",
    "\\\n",
    "But we don't know the true action values at any given time. \n",
    "\n",
    "We can estimate the value of an action as $Q_t(a)$. The closer $Q_t(a)$ is to $q_*(a)$ the better.\n",
    "\n",
    "  ### Sample Average method\n",
    "    \n",
    "  $$Q_t(a) \\doteq \\frac{\\text{sum of rewards when action a taken prior to timestep t}}{\\text{number of times action a taken prior to timestep t}}$$\n",
    "  \n",
    "  Then, the greedy action would be $A_t = \\underset{a}{argmax}\\ Q_t(a)$\n",
    "  \n",
    "  As the denominator grows larger, i.e., we obtain more and more information about action $a$, $Q_t(a)$ converges to $q_*(a)$.\n",
    "  \n",
    "  ### Incremental updates\n",
    "  \n",
    "  For a particular action, let $n$ be the number of times it was selected previously. Then its estimated *action value* before it is selected the next time is given as:\n",
    "  $$ Q_{n+1} = \\frac{1}{n}\\sum_{i=1}^{n}R_i $$\n",
    "  \n",
    "  It is better to have this in a recursive form so that we don't have to remember all $n$ counts of rewards $R_i$ to calculate the action-value at any given point\n",
    "  \n",
    "  $$ Q_{n+1} = \\frac{1}{n} \\left[ R_n + (n-1).\\frac{1}{(n-1)}\\sum_{i=1}^{n-1}R_i \\right]$$\n",
    "  \n",
    "  $$ Q_{n+1} = \\frac{1}{n} \\left[ R_n + (n-1)Q_n \\right] $$\n",
    "\n",
    "  $$ Q_{n+1} = Q_n + \\frac{1}{n} \\left( R_n - Q_n \\right) $$\n",
    "  \n",
    "  This is of the form: __NewEstimate = OldEstimate + StepSize(Target - OldEstimate)__\n",
    "  \n",
    "  * Generally, __StepSize__ is denoted by $\\alpha$, and in this case, $\\alpha = \\frac{1}{n}$ (*decaying step size*).\n",
    "  \n",
    "  * If we use a constant step size, then recent rewards will have more influence over current estimates than older rewards. This is especially useful when the problem is *non-stationary*, i.e., the true action-values keep changing.\n",
    "  \n",
    "  * Step size can take any value in $[0,1]$.\n",
    "\n",
    "## Exploration vs Exploitation\n",
    "\n",
    "In Reinforcement Learning, we have to balance exploration of the problem and exploitation of our current knowledge.\n",
    "\n",
    "__Exploitation__: we choose a greedy action based on our current estimate of the action value\n",
    "* the right thing to do when we want the highest reward at a time step.\n",
    "* might keep the agent from finding better rewards since it sticks to current knowledge\n",
    "\n",
    "__Exploration__: we select one of the non-greedy actions\n",
    "* the right thing to do when the agent has less knowledge of action-value estimates\n",
    "* can produce a greater total reward in the long\n",
    "\n",
    "## Strategies to force Exploration\n",
    "\n",
    "  ### $\\epsilon$-greedy action selection\n",
    "  \n",
    "  Since the greedy action always chooses the action with the highest value, other actions will not be sampled well enough (or not at all). A simple alternative is to choose uniformly at random any of the actions with $\\epsilon$ probability and the greedy action the rest of the time, i.e., with ($1 - \\epsilon$) probability.\n",
    "\n",
    "  ### Optimistic Initial Values\n",
    "  \n",
    "  Another simple way to ensure exploration is to initialize all action-values to a large number (usually greater than the maximum possible action value). This makes sure that all of the actions are sampled before the optimal action is identified.\n",
    "  \n",
    "  ### Upper Confidence Bound\n",
    "  \n",
    "  This method uses the principle of optimism in the face of uncertainty.\n",
    "  \n",
    "  We become more certain of the Q-value for an action the more times we sample for that action. If there is an action that has a low expected reward, but hasn't been sampled many times, and therefore, has a chance of yielding a high reward, we choose it.\n",
    "  \n",
    "  Doing so either confirms a high reward or it confirms a low reward. Either way, we become more certain of its true value.\n",
    "  \n",
    "  Sutton and Barto give us the following formula for action selection:\n",
    "  \n",
    "  $$ A_t \\doteq argmax \\left[ Q_t(a) + c \\sqrt{\\frac{ln\\ t}{N_t(a)}} \\right ] $$\n",
    "  \n",
    "  Where the first term exploits current action-values, and the second term creates exploration, and:\n",
    "  \\\n",
    "  $t$ $-$ number of time steps\n",
    "  \\\n",
    "  $N_t(a)$ $-$ number of times action a was selected\n",
    "  \\\n",
    "  $c$ $-$ a constant to adjust the exploration term\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
