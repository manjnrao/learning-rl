{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning and Learning with Tabular methods\n",
    "\n",
    "## Models\n",
    "\n",
    "A __model__ can be anything that an agent can use to predict how the environment will respond to its actions. It can be used to simulate experiences.\n",
    "\n",
    "While Dynamic Programming assumes full knowledge of the environment dynamics, TD and MC methods do not need a model of the environment, and learn from sampled experience. They are called *model-based* and *model-free* reinforcement learning methods respectively.\n",
    "\n",
    "__Model-based__ reinforcement learning methods make use of a model, and rely on *planning* as their primary component as opposed to *learning* that is the primary components in *model-free* reinforcement learning methods.\n",
    "\n",
    "* __Sample models__ return samples from a probability distribution.\n",
    "* __Distribution models__ describe the probability distribution of all possibilities.\n",
    "\n",
    "### State-space Planning\n",
    "\n",
    "*State-space planning* methods compute value functions as a key intermediate step in policy improvement.\n",
    "\n",
    "Both planning and learning methods estimate value functions with update operations (also called *back-up operations*).\n",
    "Whereas in learning we use real experiences, in planning we use simulated experiences.\n",
    "Learning methods that learn from real experiences can in many cases be applied to planning with simulated experiences.\n",
    "\n",
    "## Dyna Architecture\n",
    "\n",
    "Within an agent, there can be two roles for real experiences:\n",
    "* Improve the model (model learning)\n",
    "* Improve value function estimates (direct RL)\n",
    "\n",
    "The interactions between policy, value functions, model and experiences are shown in the diagram.\n",
    "\n",
    "![Value?policy - Model - Experience interaction](http://incompleteideas.net/book/first/ebook/figtmp63.png)\n",
    "\n",
    "Indirect methods make a fuller use of a limited amount of experience \n",
    "Achieve better policies with fewer interactions with the environment\n",
    "\n",
    "Direct methods are unaffected by bias in the model \n",
    "\n",
    "![Dyna Architecture diagram](http://incompleteideas.net/book/first/ebook/figtmp64.png)\n",
    "\n",
    "Diagram shows experience being used for both direct RL and model learning.\n",
    "search control - process by which the starting state action pairs are chosen\n",
    "planning is achieved by applying learning methods on simulated experiences as if they were real.\n",
    "Learning and Planning both share the same RL methods and differ only in the source of their experience\n",
    "\n",
    "## Changing Environments\n",
    "\n",
    "When the environment is stochastic, and only few experiences have been recorded, the model might be incomplete. If the environment is dynamic, and state transition probabilities vary, and the model becomes inaccurate. When this happens, the planning process will likely compute a suboptimal policy.\n",
    "\n",
    "### Dyna-Q+ agent\n",
    "\n",
    "The Dyna-Q+ agent solves the problem of inaccurate and incomplete models by exploration. \n",
    "\n",
    "Since exploration after computing an optimal policy results is lower return, we must balance exploration.\n",
    "\n",
    "Dyna-Q+ agent achieves this by keeping track of the time that has elapsed since a state-action pair was visited, ans adds the following term to the reward for visiting a state-action pair.\n",
    "\n",
    "$$ \\kappa \\sqrt{\\tau} $$\n",
    "\n",
    "where \n",
    "\\\n",
    "$\\tau$ is timesteps since last visit to state-action pair (s,a)\n",
    "\\\n",
    "$\\kappa$ is a small number that controls exploration.\n",
    "\n",
    "### Prioritized Sweeping\n",
    "\n",
    "Sampling experiences uniformly at random during planning is not optimal, since some states have more information or are more relevant to the problem than others. \n",
    "\n",
    "Since, the states that preceed a state whose value has changed recently are the states whose values will change during an update, it is favourable to sample those states. This idea is called *backward focusing* of planning computations.\n",
    "\n",
    "In *prioritized sweeping*, we use backward focusing of planning computation and prioritize states by the change in their value. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
