{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "Temporal Difference learning combines the positives of Dynamic programming and Monte Carlo methods.\n",
    "\\\n",
    "We combine sampling from MC and bootstrapping from DP.\n",
    "\n",
    "Consider the incremental update rule: __NewEstimate = OldEstimate + StepSize(Target - OldEstimate)__.\n",
    "\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\left [ G_t - V(S_t) \\right ] $$\n",
    "\n",
    "__Bootstrap__ the current estimate of the expected return $G_{t} = R_{t+1} + \\gamma . V(S_{t+1})$\n",
    "\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\left [ R_{t+1} + \\gamma . G_{t+1} - V(S_t) \\right ] $$\n",
    "\n",
    "$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\left [ R_{t+1} + \\gamma . V(S_{t+1}) - V(S_t) \\right ] $$\n",
    "\n",
    "__TD error__ $-$ $ \\delta = R_{t+1} + \\gamma . V(S_{t+1}) - V(S_t) $\n",
    "\\\n",
    "__TD target__ $-$ is $ R_{t+1} + \\gamma . V(S_{t+1}) $\n",
    "\n",
    "* TD, unlike DP does not require the environment dynamics\n",
    "* TD, unlike MC does not need to wait until the episode terminates\n",
    "* TD converges faster than MC\n",
    "* TD(0) has been proven to converge to $v_\\pi$ for any fixed policy $\\pi$.\n",
    "\n",
    "## SARSA (TD with GPI)\n",
    "\n",
    "* On-policy\n",
    "* We need action-value functions for control\n",
    "* Sarsa is a sample based algorithm to solve the Bellman equations for action values\n",
    "\n",
    "Similar to the TD update we have above for state values $V$, for action values we have,\n",
    "\n",
    "$$ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left [ R_{t+1} + \\gamma . Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \\right ] $$\n",
    "\n",
    "Note that it is similar to __Bellman equation__ for action value estimation.\n",
    "\n",
    "$$ q_{\\pi}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\sum_{a'} \\pi (a'|s') . q_{\\pi}(s',a') \\right ] $$\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "* Off-policy (target policy is greedy but the behavior policy need not be)\n",
    "* Uses the Bellman optimality equation for action values\n",
    "\n",
    "$$ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left [ R_{t+1} + \\gamma . \\underset{a'}{max}\\ Q(S_{t+1},a') - Q(S_t,A_t) \\right ] $$\n",
    "\n",
    "__Bellman optimality equation__ for action values:\n",
    "\n",
    "$$ q_{\\pi_*}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\underset{a'}{max} . q_{\\pi_*}(s',a') \\right ] $$\n",
    "\n",
    "## Expected Sarsa\n",
    "\n",
    "In Sarsa, we sample the next state from the environment and the next action from the policy.\n",
    "\\\n",
    "Only the environment is unknown, so why sample from the policy?\n",
    "\n",
    "Instead of sampling from its policy, Expected Sarsa, unlike Sarsa, computes the expectation of the next action.\n",
    "\n",
    "$$ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left [ R_{t+1} + \\gamma . \\sum_{a'} \\pi (a'|S_{t+1}) . Q(S_{t+1},a') - Q(S_t,A_t) \\right ] $$\n",
    "\n",
    "Note that it is still similar to __Bellman equation__ for action value estimation.\n",
    "\n",
    "$$ q_{\\pi}(s,a) = \\sum_{s',r} p(s',r | s,a) \\left [ r + \\gamma . \\sum_{a'} \\pi (a'|s') . q_{\\pi}(s',a') \\right ] $$\n",
    "\n",
    "* Expected Sarsa update targets have low variance.\n",
    "* Large step sizes can be used since the randomness within the policy is averaged.\n",
    "* More expensive if there are more actions since the average is computed at every update step.\n",
    "* Since we average over all actions, and the selected action does not matter, we can use a different policy for action selection.\n",
    "\\\n",
    "  (Expected Sarsa can be __Off-policy__).\n",
    "* Q-learning is a special case of Expected Sarsa when the target policy is greedy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
