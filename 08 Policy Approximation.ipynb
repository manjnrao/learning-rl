{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Approximation\n",
    "\n",
    "We can do *__control using action-value functions__* and a policy that selects actions based on estimated action values. \n",
    "\n",
    "We can also *__directly learn a parameterized policy__* that selects actions without consulting an action value function.\n",
    "\n",
    "$$ \\pi(a|s, \\theta) \\geq 0\\ \\ \\forall a \\in A, s \\in S $$\n",
    "\n",
    "$$ \\sum_{a\\in A} \\pi(a|s,\\theta) = 1\\ \\ \\forall s \\in S $$\n",
    "\n",
    "If the action space is discrete and not too large we can parameterize the policy using parameterized numerical preferences $h(s,a,\\theta)$ for each state-action pair, and select actions using a softmax policy.\n",
    "\n",
    "$$\\pi(a|s,\\theta) \\doteq \\frac{e^{h(s,a,\\theta)}}{\\sum_{b \\in A}e^{h(s,b,\\theta)}} $$\n",
    "\n",
    "#### Why prefer Policy Approximation?\n",
    "* The policy can be more exploratory in the beginning and the agent can learn to be greedy over time.\n",
    "* Stochastic policies are possible.\n",
    "* Can be simpler than computing action-values.\n",
    "* Stronger convergence guarantees are available for Policy gradient methods than action-value methods.\n",
    "\n",
    "## Policy Gradient Theorem\n",
    "\n",
    "Let us define the performance measure $ J(\\theta) $ in an episodic task as the value of the start state of the episode.\n",
    "\n",
    "$$ J(\\theta) \\doteq v_{\\pi_{\\theta}}(s_0) $$\n",
    "\n",
    "The policy gradient theorem gives us the gradient of the performance measure, which we can then use to perform gradient ascent to maximize the performance.\n",
    "\n",
    "$$ \\bigtriangledown J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a \\bigtriangledown \\pi(a|s,\\theta).q_\\pi(s,a) $$\n",
    "\n",
    "For episodic tasks, the proportionality constant is the average length of an episode.\n",
    "\\\n",
    "For continuing tasks, the proportionality constant is 1.\n",
    "\n",
    "## REINFORCE: Monte Carlo Policy Gradient\n",
    "\n",
    "The policy parameter update equation for gradient ascent is:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\alpha \\widehat{\\bigtriangledown J(\\theta_t)} $$\n",
    "\n",
    "where $ \\widehat{\\bigtriangledown J(\\theta_t)} $ is a stochastic estimate whose expectation approximates $\\bigtriangledown J(\\theta_t)$ $-$ the gradient of $J(\\theta_t)$ with respect to $ \\theta_t $.\n",
    "\n",
    "The policy gradient theorem gives us an expression proportional to the gradient $\\bigtriangledown J(\\theta_t)$. \n",
    "\\\n",
    "All we need is a way of sampling whose expectation equals or approximates $\\bigtriangledown J(\\theta_t)$.\n",
    "\n",
    "$$ \\bigtriangledown J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a \\bigtriangledown \\pi(a|s,\\theta).q_\\pi(s,a) $$\n",
    "\n",
    "We know that $\\mu(s)$ represents how often the states occur.\n",
    "\n",
    "$$ \\bigtriangledown J(\\theta) =  \\mathop{{}\\mathbb{E}_\\pi} \\left [ \\sum_a \\bigtriangledown \\pi(a|S_t,\\theta).q_\\pi(S_t,a) \\right ] $$\n",
    "\n",
    "Using this, we can write the update equation as:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\alpha \\sum_a \\bigtriangledown \\pi(a|S_t,\\theta).\\hat{q}(S_t,a, \\textbf{w}) $$\n",
    "\n",
    "where $\\hat{q}(S_t,a, \\textbf{w})$ is a learned approximation to $q_\\pi$.\n",
    "\n",
    "Now, we have to sample actions the same way we did for states (get $A_t$ in there in place of a, and remove $\\sum_a$)\n",
    "\\\n",
    "We need each $q$ term weighted by $\\pi(a|S_t,\\theta)$. Let's multiply and divide\n",
    "\n",
    "$$ \\bigtriangledown J(\\theta) =  \\mathop{{}\\mathbb{E}_\\pi} \\left [ \\sum_a \\bigtriangledown \\pi(a|S_t,\\theta).q_\\pi(S_t,a) \\frac{\\pi(a|S_t,\\theta)}{\\pi(a|S_t,\\theta)} \\right ] $$\n",
    "\n",
    "$$ \\bigtriangledown J(\\theta) =  \\mathop{{}\\mathbb{E}_\\pi} \\left [ \\sum_a q_\\pi(S_t,a).\\pi(a|S_t,\\theta). \\frac{\\bigtriangledown \\pi(a|S_t,\\theta)}{\\pi(a|S_t,\\theta)} \\right ] $$\n",
    "\n",
    "$$ \\bigtriangledown J(\\theta) =  \\mathop{{}\\mathbb{E}_\\pi} \\left [ q_\\pi(S_t, A_t) \\frac{\\bigtriangledown \\pi(A_t|S_t,\\theta)}{\\pi(A_t|S_t,\\theta)} \\right ] $$\n",
    "\n",
    "Since $q_\\pi(S_t, A_t) = \\mathop{{}\\mathbb{E}_\\pi} [G_t | S_t, A_t]$,\n",
    "and $\\bigtriangledown ln\\ f(x) = \\frac{\\bigtriangledown f(x)}{f(x)}$,\n",
    "\n",
    "$$ \\bigtriangledown J(\\theta) =  \\mathop{{}\\mathbb{E}_\\pi} \\left [ G_t \\bigtriangledown ln\\ \\pi(A_t|S_t,\\theta) \\right ] $$\n",
    "\n",
    "So, the update equation becomes:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\alpha\\ G_t \\bigtriangledown ln\\ \\pi(A_t|S_t,\\theta) $$\n",
    "\n",
    "## REINFORCE with Baseline\n",
    "\n",
    "We can include a baseline $b(s)$ in the policy gradient theorem as follows:\n",
    "\n",
    "$$ \\bigtriangledown J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a \\bigtriangledown \\pi(a|s,\\theta) \\bigl(q_\\pi(s,a) - b(s) \\bigr ) $$\n",
    "\n",
    "The baseline can be any function __as long as it does not depend on $a$__.\n",
    "\n",
    "Consider $\\sum_a b(s) \\bigtriangledown \\pi(a|s,\\theta)$\n",
    "\n",
    "$$ \\sum_a b(s) \\bigtriangledown \\pi(a|s,\\theta) = b(s) \\sum_a \\bigtriangledown \\pi(a|s,\\theta) = b(s) \\bigtriangledown 1 = 0 $$\n",
    "\n",
    "So, b(s) __doesn't change the expectation__ as long as it does not depend on $a$. However, it has a __large effect on its variance__.\n",
    "\n",
    "The update equation for the generalization of REINFORCE is:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\alpha\\ \\bigl(G_t - b(s) \\bigr) \\bigtriangledown ln\\ \\pi(A_t|S_t,\\theta) $$\n",
    "\n",
    "One common choice for the baseline is the estimation of the state-value function $\\hat{v}(S_t, \\textbf{w})$.\n",
    "\n",
    "## Actor-Critic Methods\n",
    "\n",
    "Actor-Critic methods have two components:\n",
    "* __Actor__: takes an action based on the policy\n",
    "* __Critic__: provides feedback to the actor by estimating the value of the states\n",
    "\n",
    "The value fucntion in REINFORCE is a baseline not a critic. We don't use it for bootstrapping. So, REINFORCE with Baseline is not an actor-critic method.\n",
    "\n",
    "REINFORCE with Baseline will converge aymptotically to a local minimum, but like all Monte Carlo methods it takes too long. It cannot be used to solve online or continuing problems.\n",
    "\n",
    "For faster learning, like in TD, we have to introduce bootstrapping. The critic part of Actor Critic is where we will do this.\n",
    "\n",
    "From the revious section, we have for $b(s) = \\hat{v}(S_t,\\textbf{w})$,\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\alpha\\ \\bigl(G_{t} - \\hat{v}(S_t,\\textbf{w}) \\bigr) \\bigtriangledown ln\\ \\pi(A_t|S_t,\\theta) $$\n",
    "\n",
    "Let's introduce bootstrapping $ \\bigl(G_{t} = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\textbf{w}) \\bigr ) $ :\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\alpha\\ \\bigl(R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\textbf{w}) - \\hat{v}(S_t,\\textbf{w}) \\bigr) \\bigtriangledown ln\\ \\pi(A_t|S_t,\\theta) $$\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\alpha\\ \\delta_t \\bigtriangledown ln\\ \\pi(A_t|S_t,\\theta) $$\n",
    "\n",
    "Through bootstrapping, we introduce bias and an asymptotic dependence on the quality of the function approximation. However, the bias introduced through bootstrapping and reliance on state representations is beneficial because it reduces variance and accelerates learning.\n",
    "\n",
    "#### The update equations for Actor-Critic methods\n",
    "\n",
    "$$ \\textbf{w} \\leftarrow \\textbf{w} + \\alpha^{\\text{w}} . \\delta . \\bigtriangledown \\hat{v}(S, \\textbf{w}) $$\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta + \\alpha^\\theta . \\delta . \\bigtriangledown ln\\ \\pi(A_t|S_t,\\theta) $$\n",
    "\n",
    "Where usually $ \\alpha^{\\theta} < \\alpha^{\\text{w}} $.\n",
    "\n",
    "And $ \\delta = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\textbf{w}) - \\hat{v}(S_t,\\textbf{w}) $.\n",
    "\n",
    "\n",
    "## Policy Parameterization for Contnuous Actions\n",
    "\n",
    "Policy based methods offer practical ways of dealing with large or even continuous action spaces.\n",
    "\n",
    "Instead of computing learned probabilities for each action, we learn statistics of the probability distribution.\n",
    "\n",
    "### Gaussian Policies\n",
    "\n",
    "The probability density function for the normal distribution with mean $\\mu$ and standard deviation $\\sigma$ is given as:\n",
    "\n",
    "$$ p(x) \\doteq \\frac{1}{\\sigma \\sqrt{2\\pi}}exp \\bigl( - \\frac{(x-\\mu)^2}{2 \\sigma^2} \\bigr) $$\n",
    "\n",
    "To produce a policy parameterization, the policy can be defined as the normal probability density over a real-valued scalar action, with mean and standard deviation given by parametric function approximators that depend on the state.\n",
    "\n",
    "$$ p(x) \\doteq \\frac{1}{\\sigma \\sqrt{2\\pi}}exp \\bigl( - \\frac{(x-\\mu)^2}{2 \\sigma^2} \\bigr) $$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
